{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKifr/IcKEFDwj5uwT9fxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpan-das-astrophysics/deep-reinforcement-learning-Huggingface/blob/main/lunar_lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1zdAVjCQrMun"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "# Unit 1: Train Our first Deep Reinforcement Learning Agent ü§ñ\n",
        "\n",
        "![Cover](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/thumbnail.jpg)\n",
        "\n",
        "In this notebook, we'll train our **first Deep Reinforcement Learning agent** a Lunar Lander agent that will learn to **land correctly on the Moon üåï**, using [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) a Deep Reinforcement Learning library.\n",
        "\n",
        "‚¨áÔ∏è Here is an example of what **we will achieve in just a couple of minutes.** ‚¨áÔ∏è\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "FGLBFZw0rzZL",
        "outputId": "acdd82f8-9a09-4e08-a130-ae382cba2877"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bf0OxZt4r-03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The environment üéÆ\n",
        "\n",
        "- [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "### The library used üìö\n",
        "\n",
        "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)"
      ],
      "metadata": {
        "id": "x7oR6R-ZIbeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gIox0tPWsBBO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6tjI2tHQ8j"
      },
      "source": [
        "## Objectives of this notebook üèÜ\n",
        "\n",
        "At the end of the notebook, we will:\n",
        "\n",
        "- Be able to use **Gymnasium**, the environment library.\n",
        "- Be able to use **Stable-Baselines3**, the deep reinforcement learning library.\n",
        "- Be able to **push our trained agent to the Hub** with a nice video replay and an evaluation score üî•.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lqlzgIBosWDE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcQYx9ynaFMD"
      },
      "source": [
        "Let's do a small recap on what we learned in the first Unit:\n",
        "\n",
        "- Reinforcement Learning is a **computational approach to learning from actions**. We build an agent that learns from the environment by **interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.\n",
        "\n",
        "- The goal of any RL agent is to **maximize its expected cumulative reward** (also called expected return) because RL is based on the _reward hypothesis_, which is that all goals can be described as the maximization of an expected cumulative reward.\n",
        "\n",
        "- The RL process is a **loop that outputs a sequence of state, action, reward, and next state**.\n",
        "\n",
        "- To calculate the expected cumulative reward (expected return), **we discount the rewards**: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long-term future reward.\n",
        "\n",
        "- To solve an RL problem, we want to **find an optimal policy**; the policy is the \"brain\" of our AI that will tell us what action to take given a state. The optimal one is the one that gives us the actions that max the expected return.\n",
        "\n",
        "There are **two** ways to find our optimal policy:\n",
        "\n",
        "- By **training our policy directly**: policy-based methods.\n",
        "- By **training a value function** that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.\n",
        "\n",
        "- Finally, we spoke about Deep RL because **we introduce deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name \"deep.\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a-m2H0jPs1gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the GPU üí™\n",
        "\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ],
      "metadata": {
        "id": "HqzznTzhNfAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fbJgJM7UtJqh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDAH0h0EBiG"
      },
      "source": [
        "## Install dependencies and create a virtual screen üîΩ\n",
        "\n",
        "The first step is to install the dependencies, we‚Äôll install multiple ones.\n",
        "\n",
        "- `gymnasium[box2d]`: Contains the LunarLander-v2 environment üåõ\n",
        "- `stable-baselines3[extra]`: The deep reinforcement learning library.\n",
        "- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ü§ó Hub.\n",
        "\n",
        "To make things easier, we created a script to install all these dependencies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install swig cmake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upDYObUAr1c-",
        "outputId": "46485503-54c6-4c7a-f201-9e9fb103ab21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 1,086 kB of archives.\n",
            "After this operation, 5,413 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1,081 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5,528 B]\n",
            "Fetched 1,086 kB in 2s (528 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 123105 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.1-5build1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.1-5build1_all.deb ...\n",
            "Unpacking swig (4.0.1-5build1) ...\n",
            "Setting up swig4.0 (4.0.1-5build1) ...\n",
            "Setting up swig (4.0.1-5build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4Hc6utotOft",
        "outputId": "f3b0a8b0-a58e-41b2-f15b-0aa5437b3b75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3==2.0.0a5 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n",
            "  Downloading stable_baselines3-2.0.0a5-py3-none-any.whl (177 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/177.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium[box2d] (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 2))\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/925.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_sb3 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3))\n",
            "  Downloading huggingface_sb3-2.2.5-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.0.1+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.7.1)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium[box2d]->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 2))\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 2)) (4.6.3)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d]->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 2))\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d]->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 2))\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.3 (from gymnasium[box2d]->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 2))\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.* (from gymnasium[box2d]->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 2))\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub~=0.8 (from huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (1.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (16.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2783893 sha256=48bb274052ba82ad4d3141a22478dbe19d7e2a85ebfb94e98e51d7414474047a\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, farama-notifications, box2d-py, pygame, jax-jumpy, huggingface-hub, gymnasium, huggingface_sb3, stable-baselines3\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.4.0\n",
            "    Uninstalling pygame-2.4.0:\n",
            "      Successfully uninstalled pygame-2.4.0\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.28.1 huggingface-hub-0.16.4 huggingface_sb3-2.2.5 jax-jumpy-1.0.0 pygame-2.1.3 stable-baselines3-2.0.0a5 swig-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NPv3DYVqtXD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install virtual screen libraries and create and run a virtual screen üñ•"
      ],
      "metadata": {
        "id": "BEKeXQJsQCYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN5GqN61tR8G",
        "outputId": "5bc95de7-8e9d-44b6-cd2e-dee07929a3f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,866 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,346 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,369 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,604 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [40.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,670 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,230 kB]\n",
            "Fetched 14.5 MB in 6s (2,583 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libpython2-stdlib python2 python2-minimal\n",
            "Suggested packages:\n",
            "  python-tk python-numpy libgle3 python2-doc\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libpython2-stdlib python-opengl python2 python2-minimal\n",
            "0 upgraded, 5 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 621 kB of archives.\n",
            "After this operation, 6,059 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-minimal amd64 2.7.17-2ubuntu4 [27.5 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-stdlib amd64 2.7.17-2ubuntu4 [7,072 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2 amd64 2.7.17-2ubuntu4 [26.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-opengl all 3.1.0+dfsg-2build1 [486 kB]\n",
            "Fetched 621 kB in 2s (396 kB/s)\n",
            "Selecting previously unselected package python2-minimal.\n",
            "(Reading database ... 123856 files and directories currently installed.)\n",
            "Preparing to unpack .../python2-minimal_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libpython2-stdlib:amd64.\n",
            "Preparing to unpack .../libpython2-stdlib_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package python2.\n",
            "(Reading database ... 123885 files and directories currently installed.)\n",
            "Preparing to unpack .../python2_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2 (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "Preparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package python-opengl.\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-2build1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-2build1) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-3) ...\n",
            "Setting up libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2 (2.7.17-2ubuntu4) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-2build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 7,697 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libfontenc1 amd64 1:1.1.4-0ubuntu1 [14.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libxfont2 amd64 1:2.0.3-1 [91.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libxkbfile1 amd64 1:1.1.0-1 [65.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-xkb-utils amd64 7.7+5 [158 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu1 [573 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-utils amd64 1:7.7+6 [91.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xserver-common all 2:1.20.13-1ubuntu1~20.04.8 [27.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 xvfb amd64 2:1.20.13-1ubuntu1~20.04.8 [780 kB]\n",
            "Fetched 7,697 kB in 2s (3,609 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 126275 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-0ubuntu1_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.3-1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu1_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a1.20.13-1ubuntu1~20.04.8_all.deb ...\n",
            "Unpacking xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a1.20.13-1ubuntu1~20.04.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Setting up x11-xkb-utils (7.7+5) ...\n",
            "Setting up xfonts-utils (1:7.7+6) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CDME30c-tuoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so we'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**"
      ],
      "metadata": {
        "id": "TCwBTAwAW9JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "0Eo1H491tf55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w24UroIiuAd4",
        "outputId": "5953d33f-4bfe-4722-ba30-7e23e20e182b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f0c2239e680>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z9snCx_OuO6p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgpVFqyENVf"
      },
      "source": [
        "## Import the packages üì¶\n",
        "\n",
        "One additional library we import is huggingface_hub **to be able to upload and download trained models from the hub**.\n",
        "\n",
        "\n",
        "The Hugging Face Hub ü§ó works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations and other features that will allow us to easily collaborate with others.\n",
        "\n",
        "We can see here all the Deep reinforcement Learning models available hereüëâ https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium\n",
        "\n",
        "from huggingface_sb3 import load_from_hub, package_to_hub\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4GllrD5uDzA",
        "outputId": "1cecae66-24f1-408b-bf83-6fa7df7deac8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
            "/usr/local/lib/python3.10/dist-packages/jaxlib/xla_client.py:225: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n",
            "  float8_e4m3b11fnuz = ml_dtypes.float8_e4m3b11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q2EPchoJuuFL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRqRuRUl8CsB"
      },
      "source": [
        "## Understand Gymnasium and how it works ü§ñ\n",
        "\n",
        "üèã The library containing our environment is called Gymnasium.\n",
        "**We'll use Gymnasium a lot in Deep Reinforcement Learning.**\n",
        "\n",
        "Gymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https://farama.org/).\n",
        "\n",
        "The Gymnasium library provides two things:\n",
        "\n",
        "- An interface that allows us to **create RL environments**.\n",
        "- A **collection of environments** (gym-control, atari, box2D...).\n",
        "\n",
        "Let's look at an example, but first let's recall the RL loop.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"The RL process\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "glXKacY3vBGN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TzNN0bQ_j-3"
      },
      "source": [
        "At each step:\n",
        "- Our Agent receives¬†a **state (S0)**¬†from the¬†**Environment**¬†‚Äî we receive the first frame of our game (Environment).\n",
        "- Based on that¬†**state (S0),**¬†the Agent takes an¬†**action (A0)**¬†‚Äî our Agent will move to the right.\n",
        "- The environment transitions to a¬†**new**¬†**state (S1)**¬†‚Äî new frame.\n",
        "- The environment gives some¬†**reward (R1)**¬†to the Agent ‚Äî we‚Äôre not dead¬†*(Positive Reward +1)*.\n",
        "\n",
        "\n",
        "With Gymnasium:\n",
        "\n",
        "1Ô∏è‚É£ We create our environment using `gymnasium.make()`\n",
        "\n",
        "2Ô∏è‚É£ We reset the environment to its initial state with `observation = env.reset()`\n",
        "\n",
        "At each step:\n",
        "\n",
        "3Ô∏è‚É£ Get an action using our model (in our example we take a random action)\n",
        "\n",
        "4Ô∏è‚É£ Using `env.step(action)`, we perform this action in the environment and get\n",
        "- `observation`: The new state (st+1)\n",
        "- `reward`: The reward we get after executing the action\n",
        "- `terminated`: Indicates if the episode terminated (agent reach the terminal state)\n",
        "- `truncated`: Introduced with this new version, it indicates a timelimit or if an agent go out of bounds of the environment for instance.\n",
        "- `info`: A dictionary that provides additional information (depends on the environment).\n",
        "\n",
        "For more explanations check this üëâ https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
        "\n",
        "If the episode is terminated:\n",
        "- We reset the environment to its initial state with `observation = env.reset()`\n",
        "\n",
        "**Let's look at an example!** Make sure to read the code\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "# First, we create our environment called LunarLander-v2\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# Then we reset this environment\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(20):\n",
        "  # Take a random action\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "\n",
        "  # Do this action in the environment and get\n",
        "  # next_state, reward, terminated, truncated and info\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
        "  if terminated or truncated:\n",
        "      # Reset the environment\n",
        "      print(\"Environment is reset\")\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWJH9PdMvRNt",
        "outputId": "b1c35764-91f0-4e50-c571-6d5664b9cefd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action taken: 0\n",
            "Action taken: 1\n",
            "Action taken: 2\n",
            "Action taken: 1\n",
            "Action taken: 3\n",
            "Action taken: 1\n",
            "Action taken: 3\n",
            "Action taken: 1\n",
            "Action taken: 1\n",
            "Action taken: 2\n",
            "Action taken: 1\n",
            "Action taken: 3\n",
            "Action taken: 1\n",
            "Action taken: 3\n",
            "Action taken: 0\n",
            "Action taken: 0\n",
            "Action taken: 0\n",
            "Action taken: 3\n",
            "Action taken: 0\n",
            "Action taken: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zty3oCJ8vePA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIrKGGSlENZB"
      },
      "source": [
        "## Create the LunarLander environment üåõ and understand how it works\n",
        "\n",
        "### [The environment üéÆ](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "In this first tutorial, we‚Äôre going to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), **to land correctly on the moon**. To do that, the agent needs to learn **to adapt its speed and position (horizontal, vertical, and angular) to land correctly.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "üí° A good habit when we start to use an environment is to check its documentation\n",
        "\n",
        "üëâ https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "09vEfmsQvnJC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poLBgRocF9aT"
      },
      "source": [
        "Let's see what the Environment looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfumE_Elvn6T",
        "outputId": "c6a159b7-a122-44f4-f3e6-1f0cf36e5c21"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "Observation Space Shape (8,)\n",
            "Sample observation [-1.55307045e+01 -5.07202034e+01 -2.02562511e-02 -4.78614569e+00\n",
            "  1.12342693e-01  1.27556860e+00  4.03743982e-01  7.59972453e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rX4Xxjz0vwsY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, where each value contains different information about the lander:\n",
        "- Horizontal pad coordinate (x)\n",
        "- Vertical pad coordinate (y)\n",
        "- Horizontal speed (x)\n",
        "- Vertical speed (y)\n",
        "- Angle\n",
        "- Angular speed\n",
        "- If the left leg contact point has touched the land\n",
        "- If the right leg contact point has touched the land\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX7rIhpKvq4W",
        "outputId": "2ac6d67f-ab96-4803-e44d-f2d2a2e63a88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "Action Space Shape 4\n",
            "Action Space Sample 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D5aKYYcJwHeC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\n",
        "\n",
        "- Action 0: Do nothing,\n",
        "- Action 1: Fire left orientation engine,\n",
        "- Action 2: Fire the main engine,\n",
        "- Action 3: Fire right orientation engine.\n",
        "\n",
        "Reward function (the function that will gives a reward at each timestep) üí∞:\n",
        "\n",
        "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
        "-  Is increased/decreased the slower/faster the lander is moving.\n",
        "- Is decreased the more the lander is tilted (angle not horizontal).\n",
        "- Is increased by 10 points for each leg that is in contact with the ground.\n",
        "- Is decreased by 0.03 points each frame a side engine is firing.\n",
        "- Is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
        "\n",
        "An episode is **considered a solution if it scores at least 200 points.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s8rjJWM7wRaZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFD9RAFjG8aq"
      },
      "source": [
        "#### Vectorized Environment\n",
        "\n",
        "- We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = make_vec_env('LunarLander-v2', n_envs=16)"
      ],
      "metadata": {
        "id": "we_G9Z04wUlQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "41u_zwCFwb2O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgrE86r5E5IK"
      },
      "source": [
        "## Create the Model ü§ñ\n",
        "- We have studied our environment and we understood the problem: **being able to land the Lunar Lander to the Landing Pad correctly by controlling left, right and main orientation engine**. Now let's build the algorithm we're going to use to solve this Problem üöÄ.\n",
        "\n",
        "- To do so, we're going to use our first Deep RL library, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).\n",
        "\n",
        "- SB3 is a set of **reliable implementations of reinforcement learning algorithms in PyTorch**.\n",
        "\n",
        "---\n",
        "\n",
        "üí° A good habit when using a new library is to dive first on the documentation: https://stable-baselines3.readthedocs.io/en/master/ and then try some tutorials.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vHJMPBYKwtl5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qL_4HeIOrEJ"
      },
      "source": [
        "Stable-Baselines3 is easy to set up:\n",
        "\n",
        "1Ô∏è‚É£ We **create our environment** (in our case it was done above)\n",
        "\n",
        "2Ô∏è‚É£ We define the **model we want to use and instantiate this model** `model = PPO(\"MlpPolicy\")`\n",
        "\n",
        "3Ô∏è‚É£ We **train the agent** with `model.learn` and define the number of training timesteps\n",
        "\n",
        "```\n",
        "# Create environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "# Instantiate the agent\n",
        "model = PPO('MlpPolicy', env, verbose=1)\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=int(2e5))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO(\n",
        "    policy = 'MlpPolicy',\n",
        "    env = env,\n",
        "    n_steps = 1024,\n",
        "    batch_size = 64,\n",
        "    n_epochs = 4,\n",
        "    gamma = 0.999,\n",
        "    gae_lambda = 0.98,\n",
        "    ent_coef = 0.01,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_wFCL_iwWFa",
        "outputId": "1262fe0b-1946-4ba6-d34e-c1feb6591b75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nTOtyZpPxQmg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJJk88yoBUi"
      },
      "source": [
        "## Train the PPO agent üèÉ\n",
        "- Let's train our agent for 1,000,000 timesteps, don't forget to use GPU on Colab. It will take approximately ~20min, but you can use fewer timesteps if you just want to try it out.\n",
        "- During the training, let's take a ‚òï break. We deserved it ü§ó"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLUTION\n",
        "# Train it for 1,000,000 timesteps\n",
        "model.learn(total_timesteps=1000000)\n",
        "# Save the model\n",
        "model_name = \"ppo-LunarLander-v2\"\n",
        "model.save(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQcOASgPxaTk",
        "outputId": "ec7feadc-7c2a-4b67-9bdb-200bf7b87a89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 91.7     |\n",
            "|    ep_rew_mean     | -194     |\n",
            "| time/              |          |\n",
            "|    fps             | 2366     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 6        |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 97.8         |\n",
            "|    ep_rew_mean          | -147         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1986         |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 16           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071847662 |\n",
            "|    clip_fraction        | 0.0464       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -0.000874    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.6e+03      |\n",
            "|    n_updates            | 4            |\n",
            "|    policy_gradient_loss | -0.00554     |\n",
            "|    value_loss           | 5.32e+03     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 106          |\n",
            "|    ep_rew_mean          | -142         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1816         |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073253876 |\n",
            "|    clip_fraction        | 0.0269       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | -0.00534     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 691          |\n",
            "|    n_updates            | 8            |\n",
            "|    policy_gradient_loss | -0.00388     |\n",
            "|    value_loss           | 1.96e+03     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 101          |\n",
            "|    ep_rew_mean          | -112         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1844         |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 35           |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059224456 |\n",
            "|    clip_fraction        | 0.0319       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.35        |\n",
            "|    explained_variance   | -0.000734    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 622          |\n",
            "|    n_updates            | 12           |\n",
            "|    policy_gradient_loss | -0.00451     |\n",
            "|    value_loss           | 1.45e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 101         |\n",
            "|    ep_rew_mean          | -105        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1801        |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 45          |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009467471 |\n",
            "|    clip_fraction        | 0.084       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.33       |\n",
            "|    explained_variance   | -0.000263   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 519         |\n",
            "|    n_updates            | 16          |\n",
            "|    policy_gradient_loss | -0.00509    |\n",
            "|    value_loss           | 955         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 106         |\n",
            "|    ep_rew_mean          | -85.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1758        |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 55          |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010074672 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | -1.45e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 232         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00645    |\n",
            "|    value_loss           | 663         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 106         |\n",
            "|    ep_rew_mean          | -68.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1776        |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 64          |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007875921 |\n",
            "|    clip_fraction        | 0.0619      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | -0.00174    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 166         |\n",
            "|    n_updates            | 24          |\n",
            "|    policy_gradient_loss | -0.00397    |\n",
            "|    value_loss           | 512         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 113         |\n",
            "|    ep_rew_mean          | -61.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1751        |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 74          |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011314103 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.26       |\n",
            "|    explained_variance   | 9.54e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 196         |\n",
            "|    n_updates            | 28          |\n",
            "|    policy_gradient_loss | -0.00667    |\n",
            "|    value_loss           | 458         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 124         |\n",
            "|    ep_rew_mean          | -43.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1709        |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 86          |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006356043 |\n",
            "|    clip_fraction        | 0.0618      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | -0.000316   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 268         |\n",
            "|    n_updates            | 32          |\n",
            "|    policy_gradient_loss | -0.00393    |\n",
            "|    value_loss           | 589         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 125         |\n",
            "|    ep_rew_mean          | -35         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1691        |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 96          |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007872937 |\n",
            "|    clip_fraction        | 0.0592      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | -6.46e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 182         |\n",
            "|    n_updates            | 36          |\n",
            "|    policy_gradient_loss | -0.00551    |\n",
            "|    value_loss           | 422         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 164         |\n",
            "|    ep_rew_mean          | -19.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1605        |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 112         |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010447858 |\n",
            "|    clip_fraction        | 0.0585      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | -0.000126   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 214         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00285    |\n",
            "|    value_loss           | 494         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 207         |\n",
            "|    ep_rew_mean          | -15.2       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1572        |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 125         |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009623431 |\n",
            "|    clip_fraction        | 0.0496      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | -2.17e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 227         |\n",
            "|    n_updates            | 44          |\n",
            "|    policy_gradient_loss | -0.00165    |\n",
            "|    value_loss           | 559         |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 220       |\n",
            "|    ep_rew_mean          | -18.5     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 1461      |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 145       |\n",
            "|    total_timesteps      | 212992    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0062108 |\n",
            "|    clip_fraction        | 0.0395    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.15     |\n",
            "|    explained_variance   | -1.25e-05 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 343       |\n",
            "|    n_updates            | 48        |\n",
            "|    policy_gradient_loss | -0.00152  |\n",
            "|    value_loss           | 656       |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 295         |\n",
            "|    ep_rew_mean          | -3.87       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1370        |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 167         |\n",
            "|    total_timesteps      | 229376      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006241495 |\n",
            "|    clip_fraction        | 0.0362      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 2.98e-06    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 181         |\n",
            "|    n_updates            | 52          |\n",
            "|    policy_gradient_loss | -0.00164    |\n",
            "|    value_loss           | 577         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 364          |\n",
            "|    ep_rew_mean          | 3.32         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1301         |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 188          |\n",
            "|    total_timesteps      | 245760       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051521957 |\n",
            "|    clip_fraction        | 0.0301       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.19        |\n",
            "|    explained_variance   | 1.1e-05      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 283          |\n",
            "|    n_updates            | 56           |\n",
            "|    policy_gradient_loss | -0.00182     |\n",
            "|    value_loss           | 421          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 424         |\n",
            "|    ep_rew_mean          | 3.03        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1246        |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 210         |\n",
            "|    total_timesteps      | 262144      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002982913 |\n",
            "|    clip_fraction        | 0.00661     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 0.138       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 225         |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00163    |\n",
            "|    value_loss           | 440         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 516          |\n",
            "|    ep_rew_mean          | 5.81         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1184         |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 235          |\n",
            "|    total_timesteps      | 278528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040738503 |\n",
            "|    clip_fraction        | 0.00764      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.405        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 97.7         |\n",
            "|    n_updates            | 64           |\n",
            "|    policy_gradient_loss | -0.00172     |\n",
            "|    value_loss           | 300          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 588          |\n",
            "|    ep_rew_mean          | 17.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1139         |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 258          |\n",
            "|    total_timesteps      | 294912       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069905515 |\n",
            "|    clip_fraction        | 0.02         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.2         |\n",
            "|    explained_variance   | 0.65         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 92.7         |\n",
            "|    n_updates            | 68           |\n",
            "|    policy_gradient_loss | -0.00334     |\n",
            "|    value_loss           | 179          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 656          |\n",
            "|    ep_rew_mean          | 29.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1102         |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 282          |\n",
            "|    total_timesteps      | 311296       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043151816 |\n",
            "|    clip_fraction        | 0.0212       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.16        |\n",
            "|    explained_variance   | 0.743        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 78.9         |\n",
            "|    n_updates            | 72           |\n",
            "|    policy_gradient_loss | -0.00205     |\n",
            "|    value_loss           | 162          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 673          |\n",
            "|    ep_rew_mean          | 36.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1078         |\n",
            "|    iterations           | 20           |\n",
            "|    time_elapsed         | 303          |\n",
            "|    total_timesteps      | 327680       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056313835 |\n",
            "|    clip_fraction        | 0.0289       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.13        |\n",
            "|    explained_variance   | 0.749        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 76.6         |\n",
            "|    n_updates            | 76           |\n",
            "|    policy_gradient_loss | -0.00191     |\n",
            "|    value_loss           | 193          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 712         |\n",
            "|    ep_rew_mean          | 49.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1052        |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 326         |\n",
            "|    total_timesteps      | 344064      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005654907 |\n",
            "|    clip_fraction        | 0.042       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.12       |\n",
            "|    explained_variance   | 0.777       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 60.6        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00178    |\n",
            "|    value_loss           | 203         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 694         |\n",
            "|    ep_rew_mean          | 54.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1034        |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 348         |\n",
            "|    total_timesteps      | 360448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005081866 |\n",
            "|    clip_fraction        | 0.0348      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.12       |\n",
            "|    explained_variance   | 0.855       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 39.4        |\n",
            "|    n_updates            | 84          |\n",
            "|    policy_gradient_loss | -0.00197    |\n",
            "|    value_loss           | 116         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 705          |\n",
            "|    ep_rew_mean          | 60.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1015         |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 371          |\n",
            "|    total_timesteps      | 376832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061263116 |\n",
            "|    clip_fraction        | 0.0479       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0.829        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 44.2         |\n",
            "|    n_updates            | 88           |\n",
            "|    policy_gradient_loss | -0.00184     |\n",
            "|    value_loss           | 162          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 728         |\n",
            "|    ep_rew_mean          | 67.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1000        |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 392         |\n",
            "|    total_timesteps      | 393216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005983918 |\n",
            "|    clip_fraction        | 0.0322      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0.851       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 90.6        |\n",
            "|    n_updates            | 92          |\n",
            "|    policy_gradient_loss | -0.00172    |\n",
            "|    value_loss           | 140         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 746         |\n",
            "|    ep_rew_mean          | 74.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 972         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 421         |\n",
            "|    total_timesteps      | 409600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005524742 |\n",
            "|    clip_fraction        | 0.0454      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.915       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 36.7        |\n",
            "|    n_updates            | 96          |\n",
            "|    policy_gradient_loss | -0.000651   |\n",
            "|    value_loss           | 66.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 792         |\n",
            "|    ep_rew_mean          | 85.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 953         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 446         |\n",
            "|    total_timesteps      | 425984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004985311 |\n",
            "|    clip_fraction        | 0.0476      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0.879       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.9        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00285    |\n",
            "|    value_loss           | 81.8        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 807          |\n",
            "|    ep_rew_mean          | 92           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 941          |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 469          |\n",
            "|    total_timesteps      | 442368       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027277218 |\n",
            "|    clip_fraction        | 0.0379       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 0.943        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.43         |\n",
            "|    n_updates            | 104          |\n",
            "|    policy_gradient_loss | -0.00109     |\n",
            "|    value_loss           | 34.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 871         |\n",
            "|    ep_rew_mean          | 105         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 926         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 495         |\n",
            "|    total_timesteps      | 458752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005310933 |\n",
            "|    clip_fraction        | 0.0353      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0.893       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.76        |\n",
            "|    n_updates            | 108         |\n",
            "|    policy_gradient_loss | -0.00114    |\n",
            "|    value_loss           | 87.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 898         |\n",
            "|    ep_rew_mean          | 107         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 913         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 520         |\n",
            "|    total_timesteps      | 475136      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005857405 |\n",
            "|    clip_fraction        | 0.0506      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0.966       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.61        |\n",
            "|    n_updates            | 112         |\n",
            "|    policy_gradient_loss | -0.00109    |\n",
            "|    value_loss           | 27          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 904         |\n",
            "|    ep_rew_mean          | 109         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 902         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 544         |\n",
            "|    total_timesteps      | 491520      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004392001 |\n",
            "|    clip_fraction        | 0.0359      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0.933       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 24.5        |\n",
            "|    n_updates            | 116         |\n",
            "|    policy_gradient_loss | -0.00124    |\n",
            "|    value_loss           | 55.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 913         |\n",
            "|    ep_rew_mean          | 112         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 892         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 568         |\n",
            "|    total_timesteps      | 507904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005474425 |\n",
            "|    clip_fraction        | 0.0462      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0.96        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 20          |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.00235    |\n",
            "|    value_loss           | 38.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 897          |\n",
            "|    ep_rew_mean          | 109          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 881          |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 595          |\n",
            "|    total_timesteps      | 524288       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069891657 |\n",
            "|    clip_fraction        | 0.0847       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | 0.985        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.81         |\n",
            "|    n_updates            | 124          |\n",
            "|    policy_gradient_loss | -0.000831    |\n",
            "|    value_loss           | 10.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 899          |\n",
            "|    ep_rew_mean          | 112          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 874          |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 618          |\n",
            "|    total_timesteps      | 540672       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058560586 |\n",
            "|    clip_fraction        | 0.0231       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 0.923        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11           |\n",
            "|    n_updates            | 128          |\n",
            "|    policy_gradient_loss | -0.00208     |\n",
            "|    value_loss           | 77.3         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 893         |\n",
            "|    ep_rew_mean          | 109         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 864         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 644         |\n",
            "|    total_timesteps      | 557056      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005718963 |\n",
            "|    clip_fraction        | 0.0485      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.967       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.2         |\n",
            "|    n_updates            | 132         |\n",
            "|    policy_gradient_loss | -0.00181    |\n",
            "|    value_loss           | 30.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 893         |\n",
            "|    ep_rew_mean          | 111         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 856         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 669         |\n",
            "|    total_timesteps      | 573440      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007002506 |\n",
            "|    clip_fraction        | 0.0432      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.963       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 41          |\n",
            "|    n_updates            | 136         |\n",
            "|    policy_gradient_loss | -0.00126    |\n",
            "|    value_loss           | 41.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 885         |\n",
            "|    ep_rew_mean          | 112         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 849         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 693         |\n",
            "|    total_timesteps      | 589824      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004920971 |\n",
            "|    clip_fraction        | 0.0457      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.998      |\n",
            "|    explained_variance   | 0.944       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 48.7        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.00119    |\n",
            "|    value_loss           | 50.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 899         |\n",
            "|    ep_rew_mean          | 114         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 844         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 717         |\n",
            "|    total_timesteps      | 606208      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004297016 |\n",
            "|    clip_fraction        | 0.0456      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.958      |\n",
            "|    explained_variance   | 0.971       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.37        |\n",
            "|    n_updates            | 144         |\n",
            "|    policy_gradient_loss | -0.00241    |\n",
            "|    value_loss           | 29          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 933         |\n",
            "|    ep_rew_mean          | 123         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 839         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 741         |\n",
            "|    total_timesteps      | 622592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004173654 |\n",
            "|    clip_fraction        | 0.0463      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.988      |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.8         |\n",
            "|    n_updates            | 148         |\n",
            "|    policy_gradient_loss | -0.000314   |\n",
            "|    value_loss           | 7.88        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 945         |\n",
            "|    ep_rew_mean          | 126         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 834         |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 765         |\n",
            "|    total_timesteps      | 638976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005241054 |\n",
            "|    clip_fraction        | 0.0394      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.97       |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.88        |\n",
            "|    n_updates            | 152         |\n",
            "|    policy_gradient_loss | -0.00068    |\n",
            "|    value_loss           | 4.39        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 959         |\n",
            "|    ep_rew_mean          | 132         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 830         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 789         |\n",
            "|    total_timesteps      | 655360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005765269 |\n",
            "|    clip_fraction        | 0.0472      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.976      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.3         |\n",
            "|    n_updates            | 156         |\n",
            "|    policy_gradient_loss | 0.0004      |\n",
            "|    value_loss           | 4.07        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 968         |\n",
            "|    ep_rew_mean          | 140         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 828         |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 811         |\n",
            "|    total_timesteps      | 671744      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003242128 |\n",
            "|    clip_fraction        | 0.0343      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.936      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.2         |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | 0.000477    |\n",
            "|    value_loss           | 4.17        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 960          |\n",
            "|    ep_rew_mean          | 141          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 823          |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 835          |\n",
            "|    total_timesteps      | 688128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042181984 |\n",
            "|    clip_fraction        | 0.0229       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.967       |\n",
            "|    explained_variance   | 0.959        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 80.2         |\n",
            "|    n_updates            | 164          |\n",
            "|    policy_gradient_loss | -0.000682    |\n",
            "|    value_loss           | 64.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 960          |\n",
            "|    ep_rew_mean          | 143          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 821          |\n",
            "|    iterations           | 43           |\n",
            "|    time_elapsed         | 857          |\n",
            "|    total_timesteps      | 704512       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036328225 |\n",
            "|    clip_fraction        | 0.0231       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.926       |\n",
            "|    explained_variance   | 0.984        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17.4         |\n",
            "|    n_updates            | 168          |\n",
            "|    policy_gradient_loss | -7.99e-05    |\n",
            "|    value_loss           | 21           |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 953         |\n",
            "|    ep_rew_mean          | 144         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 818         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 880         |\n",
            "|    total_timesteps      | 720896      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004861284 |\n",
            "|    clip_fraction        | 0.0485      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.962      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.486       |\n",
            "|    n_updates            | 172         |\n",
            "|    policy_gradient_loss | -0.000167   |\n",
            "|    value_loss           | 3.53        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 938         |\n",
            "|    ep_rew_mean          | 144         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 816         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 902         |\n",
            "|    total_timesteps      | 737280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004682722 |\n",
            "|    clip_fraction        | 0.0373      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.914      |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.804       |\n",
            "|    n_updates            | 176         |\n",
            "|    policy_gradient_loss | 0.000201    |\n",
            "|    value_loss           | 15.9        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 914          |\n",
            "|    ep_rew_mean          | 144          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 813          |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 926          |\n",
            "|    total_timesteps      | 753664       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035114796 |\n",
            "|    clip_fraction        | 0.0217       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.903       |\n",
            "|    explained_variance   | 0.977        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.97         |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.000457    |\n",
            "|    value_loss           | 30.2         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 904         |\n",
            "|    ep_rew_mean          | 144         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 812         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 947         |\n",
            "|    total_timesteps      | 770048      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004243008 |\n",
            "|    clip_fraction        | 0.0339      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.891      |\n",
            "|    explained_variance   | 0.962       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.07        |\n",
            "|    n_updates            | 184         |\n",
            "|    policy_gradient_loss | -0.000698   |\n",
            "|    value_loss           | 60.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 896          |\n",
            "|    ep_rew_mean          | 143          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 809          |\n",
            "|    iterations           | 48           |\n",
            "|    time_elapsed         | 971          |\n",
            "|    total_timesteps      | 786432       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034438157 |\n",
            "|    clip_fraction        | 0.0264       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.873       |\n",
            "|    explained_variance   | 0.963        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.68         |\n",
            "|    n_updates            | 188          |\n",
            "|    policy_gradient_loss | 9.63e-05     |\n",
            "|    value_loss           | 66.7         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 870         |\n",
            "|    ep_rew_mean          | 142         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 808         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 992         |\n",
            "|    total_timesteps      | 802816      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002574223 |\n",
            "|    clip_fraction        | 0.0269      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.891      |\n",
            "|    explained_variance   | 0.973       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 48.4        |\n",
            "|    n_updates            | 192         |\n",
            "|    policy_gradient_loss | 0.000366    |\n",
            "|    value_loss           | 40          |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 863          |\n",
            "|    ep_rew_mean          | 140          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 807          |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 1014         |\n",
            "|    total_timesteps      | 819200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038963957 |\n",
            "|    clip_fraction        | 0.0346       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.878       |\n",
            "|    explained_variance   | 0.957        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 19.4         |\n",
            "|    n_updates            | 196          |\n",
            "|    policy_gradient_loss | -0.000804    |\n",
            "|    value_loss           | 72.7         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 882         |\n",
            "|    ep_rew_mean          | 139         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 805         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 1036        |\n",
            "|    total_timesteps      | 835584      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004867871 |\n",
            "|    clip_fraction        | 0.0727      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.903      |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.5        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.000867   |\n",
            "|    value_loss           | 26.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 892         |\n",
            "|    ep_rew_mean          | 136         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 805         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 1058        |\n",
            "|    total_timesteps      | 851968      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004238687 |\n",
            "|    clip_fraction        | 0.0397      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.852      |\n",
            "|    explained_variance   | 0.986       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.81        |\n",
            "|    n_updates            | 204         |\n",
            "|    policy_gradient_loss | -0.000654   |\n",
            "|    value_loss           | 17.7        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 900          |\n",
            "|    ep_rew_mean          | 138          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 804          |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 1079         |\n",
            "|    total_timesteps      | 868352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025464417 |\n",
            "|    clip_fraction        | 0.0228       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.83        |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 45.5         |\n",
            "|    n_updates            | 208          |\n",
            "|    policy_gradient_loss | -0.000508    |\n",
            "|    value_loss           | 25           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 910          |\n",
            "|    ep_rew_mean          | 136          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 804          |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 1099         |\n",
            "|    total_timesteps      | 884736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048181466 |\n",
            "|    clip_fraction        | 0.0413       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.86        |\n",
            "|    explained_variance   | 0.976        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.72         |\n",
            "|    n_updates            | 212          |\n",
            "|    policy_gradient_loss | -0.000787    |\n",
            "|    value_loss           | 29.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 947          |\n",
            "|    ep_rew_mean          | 143          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 803          |\n",
            "|    iterations           | 55           |\n",
            "|    time_elapsed         | 1121         |\n",
            "|    total_timesteps      | 901120       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054431185 |\n",
            "|    clip_fraction        | 0.0643       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.852       |\n",
            "|    explained_variance   | 0.991        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.67         |\n",
            "|    n_updates            | 216          |\n",
            "|    policy_gradient_loss | -0.000793    |\n",
            "|    value_loss           | 8.26         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 932          |\n",
            "|    ep_rew_mean          | 143          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 802          |\n",
            "|    iterations           | 56           |\n",
            "|    time_elapsed         | 1142         |\n",
            "|    total_timesteps      | 917504       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038632122 |\n",
            "|    clip_fraction        | 0.0422       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.855       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.644        |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | 0.000212     |\n",
            "|    value_loss           | 2.27         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 932         |\n",
            "|    ep_rew_mean          | 146         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 800         |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 1166        |\n",
            "|    total_timesteps      | 933888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004178867 |\n",
            "|    clip_fraction        | 0.0283      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.855      |\n",
            "|    explained_variance   | 0.959       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 21.6        |\n",
            "|    n_updates            | 224         |\n",
            "|    policy_gradient_loss | -0.00127    |\n",
            "|    value_loss           | 62          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 937         |\n",
            "|    ep_rew_mean          | 156         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 800         |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 1187        |\n",
            "|    total_timesteps      | 950272      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004941963 |\n",
            "|    clip_fraction        | 0.0492      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.839      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.751       |\n",
            "|    n_updates            | 228         |\n",
            "|    policy_gradient_loss | -0.000393   |\n",
            "|    value_loss           | 2.56        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 923         |\n",
            "|    ep_rew_mean          | 161         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 799         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 1209        |\n",
            "|    total_timesteps      | 966656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004714527 |\n",
            "|    clip_fraction        | 0.0421      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.792      |\n",
            "|    explained_variance   | 0.948       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 26.3        |\n",
            "|    n_updates            | 232         |\n",
            "|    policy_gradient_loss | -0.00146    |\n",
            "|    value_loss           | 101         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 873          |\n",
            "|    ep_rew_mean          | 169          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 799          |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 1229         |\n",
            "|    total_timesteps      | 983040       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053411713 |\n",
            "|    clip_fraction        | 0.0471       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.735       |\n",
            "|    explained_variance   | 0.952        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 90.7         |\n",
            "|    n_updates            | 236          |\n",
            "|    policy_gradient_loss | -0.00203     |\n",
            "|    value_loss           | 78.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 795          |\n",
            "|    ep_rew_mean          | 194          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 800          |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 1249         |\n",
            "|    total_timesteps      | 999424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063254647 |\n",
            "|    clip_fraction        | 0.0842       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.713       |\n",
            "|    explained_variance   | 0.881        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 74.4         |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.00319     |\n",
            "|    value_loss           | 224          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 617          |\n",
            "|    ep_rew_mean          | 218          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 801          |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 1267         |\n",
            "|    total_timesteps      | 1015808      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050476934 |\n",
            "|    clip_fraction        | 0.0794       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.667       |\n",
            "|    explained_variance   | 0.817        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 95.1         |\n",
            "|    n_updates            | 244          |\n",
            "|    policy_gradient_loss | -0.00415     |\n",
            "|    value_loss           | 356          |\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aO4T0YgWxdH_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}